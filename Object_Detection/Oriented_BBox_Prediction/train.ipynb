{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SceneTextDataset\n",
      "{'dataset_params': {'root_dir': 'Q1', 'num_classes': 2}, 'model_params': {'im_channels': 3, 'aspect_ratios': [0.5, 1, 2], 'scales': [128, 256, 512], 'min_im_size': 600, 'max_im_size': 1000, 'backbone_out_channels': 512, 'fc_inner_dim': 1024, 'rpn_bg_threshold': 0.3, 'rpn_fg_threshold': 0.7, 'rpn_nms_threshold': 0.7, 'rpn_train_prenms_topk': 12000, 'rpn_test_prenms_topk': 6000, 'rpn_train_topk': 2000, 'rpn_test_topk': 300, 'rpn_batch_size': 256, 'rpn_pos_fraction': 0.5, 'roi_iou_threshold': 0.5, 'roi_low_bg_iou': 0.1, 'roi_pool_size': 7, 'roi_nms_threshold': 0.3, 'roi_topk_detections': 100, 'roi_score_threshold': 0.05, 'roi_batch_size': 128, 'roi_pos_fraction': 0.25}, 'train_params': {'task_name': 'st', 'seed': 1111, 'infer_seed': 1122, 'acc_steps': 1, 'num_epochs': 100, 'lr_steps': [12, 16], 'lr': 0.001, 'ckpt_name': 'faster_rcnn_st.pth'}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sid/miniconda3/envs/md-intro-tutorial/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained_backbone' is deprecated since 0.13 and may be removed in the future, please use 'weights_backbone' instead.\n",
      "  warnings.warn(\n",
      "/home/sid/miniconda3/envs/md-intro-tutorial/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights_backbone' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights_backbone=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights_backbone=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholded_thetas in pred False\n",
      "OrientedBoxCoder\n",
      "(10.0, 10.0, 5.0, 5.0, 1.0)\n",
      "thresholded_thetas in pred True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/250 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (4096) to match target batch_size (2048).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 207\u001b[0m\n\u001b[1;32m    204\u001b[0m     config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconfig/st.yaml\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    206\u001b[0m args \u001b[38;5;241m=\u001b[39m Args()\n\u001b[0;32m--> 207\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 176\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(args, output_dir)\u001b[0m\n\u001b[1;32m    173\u001b[0m     target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m target[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    175\u001b[0m images \u001b[38;5;241m=\u001b[39m [im\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m im \u001b[38;5;129;01min\u001b[39;00m ims]\n\u001b[0;32m--> 176\u001b[0m batch_losses \u001b[38;5;241m=\u001b[39m \u001b[43mfaster_rcnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(batch_losses\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m    179\u001b[0m rpn_classification_losses\u001b[38;5;241m.\u001b[39mappend(batch_losses[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_objectness\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/miniconda3/envs/md-intro-tutorial/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/md-intro-tutorial/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c/Users/siddharth/assignment-3-siddharthsingh250304/q1/detection/generalized_rcnn.py:106\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets, verbose)\u001b[0m\n\u001b[1;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[1;32m    104\u001b[0m proposals, proposal_losses, objectness_maps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(images, features, targets)\n\u001b[0;32m--> 106\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroi_heads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproposals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)\n\u001b[1;32m    110\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/miniconda3/envs/md-intro-tutorial/lib/python3.13/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/md-intro-tutorial/lib/python3.13/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/c/Users/siddharth/assignment-3-siddharthsingh250304/q1/detection/roi_heads.py:368\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[0;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregression_targets cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthresholded_thetas:\n\u001b[0;32m--> 368\u001b[0m     loss_classifier, loss_box_reg, loss_theta_reg \u001b[38;5;241m=\u001b[39m \u001b[43mfastrcnn_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclass_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox_regression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregression_theta_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    369\u001b[0m     losses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_classifier\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_classifier, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_box_reg\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_box_reg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_theta_reg\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_theta_reg}\n\u001b[1;32m    370\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/siddharth/assignment-3-siddharthsingh250304/q1/detection/roi_heads.py:44\u001b[0m, in \u001b[0;36mfastrcnn_loss\u001b[0;34m(class_logits, box_regression, labels, regression_targets, theta_logits, regression_theta_logits)\u001b[0m\n\u001b[1;32m     37\u001b[0m box_regression \u001b[38;5;241m=\u001b[39m box_regression\u001b[38;5;241m.\u001b[39mreshape(N, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     38\u001b[0m box_loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msmooth_l1_loss(\n\u001b[1;32m     39\u001b[0m     box_regression[sampled_pos_inds_subset, labels_pos],\n\u001b[1;32m     40\u001b[0m     regression_targets[sampled_pos_inds_subset],\n\u001b[1;32m     41\u001b[0m     beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m9\u001b[39m,\n\u001b[1;32m     42\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msum\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m theta_loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregression_theta_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m box_loss \u001b[38;5;241m=\u001b[39m box_loss \u001b[38;5;241m/\u001b[39m labels\u001b[38;5;241m.\u001b[39mnumel()\n\u001b[1;32m     46\u001b[0m theta_loss \u001b[38;5;241m=\u001b[39m theta_loss \u001b[38;5;241m/\u001b[39m labels\u001b[38;5;241m.\u001b[39mnumel()\n",
      "File \u001b[0;32m~/miniconda3/envs/md-intro-tutorial/lib/python3.13/site-packages/torch/nn/functional.py:3494\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3492\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3493\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3494\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_nn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3498\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3499\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3500\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3501\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (4096) to match target batch_size (2048)."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import yaml\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from infer import evaluate_map_v2, infer_from_model\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from dataset.st import SceneTextDataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from detection.transform import GeneralizedRCNNTransform, resize_boxes\n",
    "import detection\n",
    "from detection.faster_rcnn import FastRCNNPredictor\n",
    "from detection.anchor_utils import AnchorGenerator\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Collate function\n",
    "def collate_function(data):\n",
    "    return tuple(zip(*data))\n",
    "\n",
    "def draw_and_save_boxes(image, boxes, labels, scores, output_dir, img_idx):\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.imshow(image.permute(1, 2, 0))\n",
    "    ax = plt.gca()\n",
    "    for box, label, score in zip(boxes, labels, scores):\n",
    "        x1, y1, x2, y2 = box\n",
    "        ax.add_patch(plt.Rectangle((x1, y1), x2 - x1, y2 - y1, fill=False, edgecolor='red', linewidth=2))\n",
    "        ax.text(x1, y1, f'{label} {score:.2f}', fontsize=8, color='red')\n",
    "    plt.axis('off')\n",
    "    plt.savefig(os.path.join(output_dir, f'img_{img_idx}_boxes.png'))\n",
    "    plt.close()\n",
    "\n",
    "def save_objectness_heatmap(objectness_scores, base_dir=\"\"):\n",
    "    num_levels = len(objectness_scores[0])  \n",
    "    num_imgs = len(objectness_scores)\n",
    "\n",
    "    os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "    for level in range(num_levels):\n",
    "        level_dir = os.path.join(base_dir, f\"level_{level}\")\n",
    "        os.makedirs(level_dir, exist_ok=True)\n",
    "\n",
    "        for img_idx in range(num_imgs):\n",
    "            heatmap = objectness_scores[img_idx][level][0]  # Select first anchor channel\n",
    "\n",
    "            plt.figure(figsize=(5, 5))\n",
    "            plt.imshow(heatmap, cmap='hot', interpolation='nearest')\n",
    "            plt.title(f\"Level {level} - Image {img_idx}\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            img_path = os.path.join(level_dir, f\"img_{img_idx}_heatmap.png\")\n",
    "            print(f\"Saving heatmap to {img_path}\")\n",
    "            plt.savefig(img_path)\n",
    "            plt.close()\n",
    "\n",
    "def train(args, output_dir='output'):\n",
    "    with open(args.config_path, 'r') as file:\n",
    "        try:\n",
    "            config = yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "            return\n",
    "    \n",
    "    print(config)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    dataset_config = config['dataset_params']\n",
    "    train_config = config['train_params']\n",
    "    \n",
    "    seed = train_config['seed']\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    if device.type == 'cuda':\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Load dataset\n",
    "    st = SceneTextDataset('train', root_dir=dataset_config['root_dir'])\n",
    "    train_indices = np.random.choice(len(st), len(st) - 4, replace=False)\n",
    "    test_indices = np.setdiff1d(np.arange(len(st)), train_indices)\n",
    "    \n",
    "    train_data = torch.utils.data.Subset(st, train_indices)\n",
    "    test_data = torch.utils.data.Subset(st, test_indices)\n",
    "    \n",
    "    train_dataset = DataLoader(train_data, batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_function)\n",
    "    test_dataset = DataLoader(test_data, batch_size=4, shuffle=False, num_workers=0, collate_fn=collate_function)\n",
    "    \n",
    "    # Model setup\n",
    "    faster_rcnn_model = detection.fasterrcnn_resnet50_fpn(pretrained_backbone=True, min_size=600, max_size=1000, thresholded_thetas=True, num_thetas=10)\n",
    "    faster_rcnn_model.roi_heads.box_predictor = FastRCNNPredictor(\n",
    "        faster_rcnn_model.roi_heads.box_predictor.cls_score.in_features,\n",
    "        num_classes=dataset_config['num_classes'],\n",
    "        thresholded_thetas=True,\n",
    "        num_thetas=10\n",
    "    )\n",
    "    \n",
    "    faster_rcnn_model.to(device)\n",
    "    os.makedirs(train_config['task_name'], exist_ok=True)\n",
    "    \n",
    "    optimizer = torch.optim.SGD(\n",
    "        lr=1E-4, params=filter(lambda p: p.requires_grad, faster_rcnn_model.parameters()),\n",
    "        weight_decay=5E-5, momentum=0.9\n",
    "    )\n",
    "    \n",
    "    num_epochs = train_config['num_epochs']\n",
    "    num_epochs = 50\n",
    "    faster_rcnn_model.train()\n",
    "    for i in range(num_epochs):\n",
    "        rpn_classification_losses = []\n",
    "        rpn_localization_losses = []\n",
    "        frcnn_classification_losses = []\n",
    "        frcnn_localization_losses = []\n",
    "        \n",
    "\n",
    "        epoch_dir = os.path.join(output_dir, 'heatmap_frames', f'epoch_{i}')\n",
    "        os.makedirs(epoch_dir, exist_ok=True)\n",
    "        \n",
    "        os.makedirs(os.path.join(output_dir, f'epoch_{i}'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(epoch_dir, 'test'), exist_ok=True)\n",
    "        os.makedirs(os.path.join(epoch_dir, 'train'), exist_ok=True)\n",
    "        infer_from_model(faster_rcnn_model, test_dataset, os.path.join(epoch_dir, 'test'))\n",
    "        mean_ap_test, pre_test, rec_test = evaluate_map_v2(faster_rcnn_model, test_dataset)\n",
    "        \n",
    "        infer_from_model(faster_rcnn_model, train_dataset, os.path.join(epoch_dir, 'train'))\n",
    "        mean_ap_train, pre_train, rec_train = evaluate_map_v2(faster_rcnn_model, train_dataset)\n",
    "        \n",
    "        # Save AP values\n",
    "        outfile_test = os.path.join(output_dir, f'epoch_{i}.txt')\n",
    "        outfile_train = os.path.join(output_dir, f'epoch_train_{i}.txt')\n",
    "        print(pre_train, pre_test)\n",
    "        with open(outfile_test, 'a') as f:\n",
    "            ctr = 0\n",
    "            for j in np.linspace(0.05, 0.95, 10):\n",
    "                avg_pre = np.mean(pre_test[ctr])\n",
    "                avg_rec = np.mean(rec_test[ctr])\n",
    "                f.write(f'threshold = {j}\\n Precision = {avg_pre:.4f} | Recall = {avg_rec:.4f}\\n mean AP = {mean_ap_test[ctr]:.4f}\\n')\n",
    "                ctr += 1\n",
    "        with open(outfile_train, 'a') as f:\n",
    "            ctr = 0\n",
    "            for j in np.linspace(0.05, 0.95, 10):\n",
    "                avg_pre = np.mean(pre_train[ctr])\n",
    "                avg_rec = np.mean(rec_train[ctr])\n",
    "                f.write(f'threshold = {j}\\n Precision = {avg_pre:.4f} | Recall = {avg_rec:.4f}\\n mean AP = {mean_ap_test[ctr]:.4f}\\n')\n",
    "                ctr += 1\n",
    "        \n",
    "        print(f\"Saved test AP to {outfile_test}\")\n",
    "        print(f\"Saved train AP to {outfile_train}\")\n",
    "        \n",
    "        #get objectness heatmap\n",
    "        epoch_dir = os.path.join(output_dir, 'heatmap_frames', f'epoch_{i}')\n",
    "        for ims, targets, _ in test_dataset:\n",
    "            images = [im.float().to(device) for im in ims]\n",
    "            with torch.no_grad():\n",
    "                transform = GeneralizedRCNNTransform(min_size=800, max_size=1333, image_mean=[0.485, 0.456, 0.406], image_std=[0.229, 0.224, 0.225])\n",
    "                image_list, _ = transform(images)\n",
    "\n",
    "                backbone_features = faster_rcnn_model.backbone(image_list.tensors)\n",
    "                \n",
    "                # Extract RPN objectness scores\n",
    "                features_list = list(backbone_features.values())  # Convert OrderedDict to list of tensors\n",
    "                proposals, proposal_losses, _ = faster_rcnn_model.rpn(image_list, features_list, targets)\n",
    "                proposals = resize_boxes(proposals, image_list.image_sizes, images[0].shape[-2:])\n",
    "                \n",
    "                rpn_logits, _ = faster_rcnn_model.rpn.head(features_list)\n",
    "                # Convert logits to probabilities\n",
    "                objectness_scores = [logit.sigmoid().cpu().numpy() for logit in rpn_logits]\n",
    "                save_objectness_heatmap(objectness_scores, epoch_dir)\n",
    "                \n",
    "    \n",
    "\n",
    "        for ims, targets, _ in tqdm(train_dataset):\n",
    "            optimizer.zero_grad()\n",
    "            images = [im.float().to(device) for im in ims]\n",
    "            \n",
    "            for target in targets:\n",
    "                target['boxes'] = target['bboxes'].float().to(device)\n",
    "                target['thetas'] = target['thetas'].float().to(device)\n",
    "                del target['bboxes']\n",
    "                target['labels'] = target['labels'].long().to(device)\n",
    "            \n",
    "            images = [im.float().to(device) for im in ims]\n",
    "            batch_losses = faster_rcnn_model(images, targets)\n",
    "            loss = sum(batch_losses.values())\n",
    "            \n",
    "            rpn_classification_losses.append(batch_losses['loss_objectness'].item())\n",
    "            rpn_localization_losses.append(batch_losses['loss_rpn_box_reg'].item())\n",
    "            frcnn_classification_losses.append(batch_losses['loss_classifier'].item())\n",
    "            frcnn_localization_losses.append(batch_losses['loss_box_reg'].item())\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        print(f'Finished epoch {i}')\n",
    "        torch.save(\n",
    "            faster_rcnn_model.state_dict(),\n",
    "            os.path.join(output_dir, f'tv_frcnn_r50fpn_{train_config[\"ckpt_name\"]}')\n",
    "        )\n",
    "        \n",
    "        loss_output = (\n",
    "            f\"RPN Classification Loss: {np.mean(rpn_classification_losses):.4f} | \"\n",
    "            f\"RPN Localization Loss: {np.mean(rpn_localization_losses):.4f} | \"\n",
    "            f\"FRCNN Classification Loss: {np.mean(frcnn_classification_losses):.4f} | \"\n",
    "            f\"FRCNN Localization Loss: {np.mean(frcnn_localization_losses):.4f}\"\n",
    "        )\n",
    "        print(loss_output)\n",
    "    \n",
    "    print('Done Training...')\n",
    "\n",
    "class Args:\n",
    "    config_path = 'config/st.yaml'\n",
    "\n",
    "args = Args()\n",
    "train(args, output_dir='output')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md-intro-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
