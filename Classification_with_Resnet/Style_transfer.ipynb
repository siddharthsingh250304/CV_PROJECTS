{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Content Loss Layer\n",
    "class ContentLoss(nn.Module):\n",
    "\tdef __init__(self, target):\n",
    "\t\tsuper(ContentLoss, self).__init__()\n",
    "\t\tself.target = target.detach()\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.loss = F.mse_loss(input, self.target)\n",
    "\t\treturn input\n",
    "\n",
    "# Style Loss Layer\n",
    "class StyleLoss(nn.Module):\n",
    "\tdef __init__(self, target_feature):\n",
    "\t\tsuper(StyleLoss, self).__init__()\n",
    "\t\tself.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tG = gram_matrix(input)\n",
    "\t\tself.loss = F.mse_loss(G, self.target)\n",
    "\t\treturn input\n",
    "\n",
    "def gram_matrix(input):\n",
    "\ta, b, c, d = input.size()\n",
    "\tfeatures = input.view(a* b, c*d)\n",
    "\tG = torch.mm(features, features.t())\n",
    "\treturn G.div(a*b*c*d)\n",
    "\n",
    "# Normalization Layer to transform input images\n",
    "class Normalization(nn.Module):\n",
    "\tdef __init__(self, mean, std):\n",
    "\t\tsuper(Normalization, self).__init__()\n",
    "\t\tself.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "\t\tself.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "\tdef forward(self, image):\n",
    "\t\treturn (image - self.mean) / self.std\n",
    "\n",
    "# Create our model with our loss layers\n",
    "def style_cnn(cnn, device, normalization_mean, normalization_std, style_image, content_image):\n",
    "\t# Insert loss layers after these desired layers\n",
    "\tstyle_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\tcontent_layers = ['conv_4']\n",
    "\t\n",
    "\t# Copy network to work on\n",
    "\tcnn = copy.deepcopy(cnn)\n",
    "\n",
    "\t# Keep track of our losses\n",
    "\tstyle_losses = []\n",
    "\tcontent_losses = []\n",
    "\n",
    "\t# Start by normalizing our image\n",
    "\tnormalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\tmodel = nn.Sequential(normalization)\n",
    "\n",
    "\t# Keep track of convolutional layers\n",
    "\ti = 0\n",
    "\n",
    "\t# Loop through vgg layers\n",
    "\tfor layer in cnn.children():\n",
    "\t\tif isinstance(layer, nn.Conv2d):\n",
    "\t\t\ti += 1\n",
    "\t\t\tname = 'conv_{}'.format(i)\n",
    "\t\telif isinstance(layer, nn.ReLU):\n",
    "\t\t\tname = 'relu_{}'.format(i)\n",
    "\t\t\tlayer = nn.ReLU(inplace=False)\n",
    "\t\telif isinstance(layer, nn.MaxPool2d):\n",
    "\t\t\tname = 'pool_{}'.format(i)\n",
    "\t\telif isinstance(layer, nn.BatchNorm2d):\n",
    "\t\t\tname = 'bn_{}'.format(i)\n",
    "\n",
    "\t\t# Add layer to our model\n",
    "\t\tmodel.add_module(name, layer)\n",
    "\n",
    "\t\t# Insert style loss layer\n",
    "\t\tif name in style_layers:\n",
    "\t\t\ttarget_feature = model(style_image).detach()\n",
    "\t\t\tstyle_loss = StyleLoss(target_feature)\n",
    "\t\t\tmodel.add_module('style_loss_{}'.format(i), style_loss)\n",
    "\t\t\tstyle_losses.append(style_loss)\n",
    "\n",
    "\t\t# Insert content loss layer\n",
    "\t\tif name in content_layers:\n",
    "\t\t\ttarget = model(content_image).detach()\n",
    "\t\t\tcontent_loss = ContentLoss(target)\n",
    "\t\t\tmodel.add_module('content_loss_{}'.format(i), content_loss)\n",
    "\t\t\tcontent_losses.append(content_loss)\n",
    "\n",
    "\t# Get rid of unneeded layers after our final losses\n",
    "\tfor i in range(len(model) - 1, -1, -1):\n",
    "\t\tif isinstance(model[i], StyleLoss) or isinstance(model[i], ContentLoss):\n",
    "\t\t\tbreak\n",
    "\n",
    "\tmodel = model[:(i + 1)]\n",
    "\n",
    "\treturn model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Processing: Q3/content/bear.jpg → Q3/content2/bear.jpg\n",
      "Processing: Q3/content/building.jpg → Q3/content2/building.jpg\n",
      "Processing: Q3/content/cat-on-table.jpg → Q3/content2/cat-on-table.jpg\n",
      "Processing: Q3/content/cat-sleeping.jpg → Q3/content2/cat-sleeping.jpg\n",
      "Processing: Q3/content/cat.jpg → Q3/content2/cat.jpg\n",
      "Processing: Q3/content/cows.jpg → Q3/content2/cows.jpg\n",
      "Processing: Q3/content/mountains.jpg → Q3/content2/mountains.jpg\n",
      "Processing: Q3/content/picnic.jpg → Q3/content2/picnic.jpg\n",
      "Processing: Q3/content/town.jpg → Q3/content2/town.jpg\n",
      "Processing: Q3/content/white-building.jpg → Q3/content2/white-building.jpg\n",
      "Processing: Q3/styles/bet-you.jpg → Q3/styles2/bet-you.jpg\n",
      "Processing: Q3/styles/Erin-Hanson-Monet's-Bridge.jpg → Q3/styles2/Erin-Hanson-Monet's-Bridge.jpg\n",
      "Processing: Q3/styles/head-of-paula-eyles.jpg → Q3/styles2/head-of-paula-eyles.jpg\n",
      "Processing: Q3/styles/horse-cart.jpg → Q3/styles2/horse-cart.jpg\n",
      "Processing: Q3/styles/landscape-with-a-palace.jpg → Q3/styles2/landscape-with-a-palace.jpg\n",
      "Processing: Q3/styles/not-detected.jpg → Q3/styles2/not-detected.jpg\n",
      "Processing: Q3/styles/red-sea-passage.jpg → Q3/styles2/red-sea-passage.jpg\n",
      "Processing: Q3/styles/the-couple.jpg → Q3/styles2/the-couple.jpg\n",
      "Processing: Q3/styles/the-exit-of-the-russian-ballet.jpg → Q3/styles2/the-exit-of-the-russian-ballet.jpg\n",
      "Processing: Q3/styles/zebra.jpg → Q3/styles2/zebra.jpg\n",
      "Processing complete! ✅\n",
      "Width: 512, Height: 512\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "\n",
    "# Set device (GPU if available, else CPU)\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# Image size\n",
    "SIZE = 512  \n",
    "\n",
    "# Define image transformations\n",
    "loader = T.Compose([\n",
    "    T.Resize(SIZE),      \n",
    "    T.CenterCrop(SIZE),  \n",
    "    T.ToTensor()         \n",
    "])\n",
    "\n",
    "unloader = T.ToPILImage()  \n",
    "\n",
    "# Function to Load and Process Image\n",
    "def load_image(path):\n",
    "    image = loader(Image.open(path).convert(\"RGB\")).unsqueeze(0)  # Convert to RGB\n",
    "    return image.to(DEVICE, torch.float)  \n",
    "\n",
    "# Function to Save Processed Image\n",
    "def save_image(tensor, path):\n",
    "    image = unloader(tensor.cpu().clone().squeeze(0))  \n",
    "    image.save(path)  \n",
    "\n",
    "# Function to Process All Images in a Folder\n",
    "def process_images(input_folder, output_folder):\n",
    "    if not os.path.exists(output_folder):  \n",
    "        os.makedirs(output_folder)  # Create output folder if it doesn’t exist\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        input_path = os.path.join(input_folder, filename)\n",
    "        output_path = os.path.join(output_folder, filename)\n",
    "\n",
    "        # Process only image files\n",
    "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif', '.tiff')):\n",
    "            print(f\"Processing: {input_path} → {output_path}\")\n",
    "            img_tensor = load_image(input_path)  \n",
    "            save_image(img_tensor, output_path)  \n",
    "\n",
    "# Define folders\n",
    "content_folder = \"Q3/content\"\n",
    "styles_folder = \"Q3/styles\"\n",
    "content_output_folder = \"Q3/content2\"\n",
    "styles_output_folder = \"Q3/styles2\"\n",
    "\n",
    "# Process content and styles images\n",
    "process_images(content_folder, content_output_folder)\n",
    "process_images(styles_folder, styles_output_folder)\n",
    "\n",
    "print(\"Processing complete! ✅\")\n",
    "\n",
    "# to check if size of new image is 512x512 \n",
    "img = Image.open(\"Q3/content2/cat.jpg\")\n",
    "width, height = img.size\n",
    "print(f\"Width: {width}, Height: {height}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Content Loss Layer\n",
    "class ContentLoss(nn.Module):\n",
    "\tdef __init__(self, target):\n",
    "\t\tsuper(ContentLoss, self).__init__()\n",
    "\t\tself.target = target.detach()\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tself.loss = F.mse_loss(input, self.target)\n",
    "\t\treturn input\n",
    "\n",
    "# Style Loss Layer\n",
    "class StyleLoss(nn.Module):\n",
    "\tdef __init__(self, target_feature):\n",
    "\t\tsuper(StyleLoss, self).__init__()\n",
    "\t\tself.target = gram_matrix(target_feature).detach()\n",
    "\n",
    "\tdef forward(self, input):\n",
    "\t\tG = gram_matrix(input)\n",
    "\t\tself.loss = F.mse_loss(G, self.target)\n",
    "\t\treturn input\n",
    "\n",
    "def gram_matrix(input):\n",
    "\ta, b, c, d = input.size()\n",
    "\tfeatures = input.view(a* b, c*d)\n",
    "\tG = torch.mm(features, features.t())\n",
    "\treturn G.div(a*b*c*d)\n",
    "\n",
    "# Normalization Layer to transform input images\n",
    "class Normalization(nn.Module):\n",
    "\tdef __init__(self, mean, std):\n",
    "\t\tsuper(Normalization, self).__init__()\n",
    "\t\tself.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "\t\tself.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "\tdef forward(self, image):\n",
    "\t\treturn (image - self.mean) / self.std\n",
    "\n",
    "# Create our model with our loss layers\n",
    "def style_cnn(cnn, device, normalization_mean, normalization_std, style_image, content_image):\n",
    "\t# Insert loss layers after these desired layers\n",
    "\tstyle_layers = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\tcontent_layers = ['conv_4']\n",
    "\t\n",
    "\t# Copy network to work on\n",
    "\tcnn = copy.deepcopy(cnn)\n",
    "\n",
    "\t# Keep track of our losses\n",
    "\tstyle_losses = []\n",
    "\tcontent_losses = []\n",
    "\n",
    "\t# Start by normalizing our image\n",
    "\tnormalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\tmodel = nn.Sequential(normalization)\n",
    "\n",
    "\t# Keep track of convolutional layers\n",
    "\ti = 0\n",
    "\n",
    "\t# Loop through vgg layers\n",
    "\tfor layer in cnn.children():\n",
    "\t\tif isinstance(layer, nn.Conv2d):\n",
    "\t\t\ti += 1\n",
    "\t\t\tname = 'conv_{}'.format(i)\n",
    "\t\telif isinstance(layer, nn.ReLU):\n",
    "\t\t\tname = 'relu_{}'.format(i)\n",
    "\t\t\tlayer = nn.ReLU(inplace=False)\n",
    "\t\telif isinstance(layer, nn.MaxPool2d):\n",
    "\t\t\tname = 'pool_{}'.format(i)\n",
    "\t\telif isinstance(layer, nn.BatchNorm2d):\n",
    "\t\t\tname = 'bn_{}'.format(i)\n",
    "\n",
    "\t\t# Add layer to our model\n",
    "\t\tmodel.add_module(name, layer)\n",
    "\n",
    "\t\t# Insert style loss layer\n",
    "\t\tif name in style_layers:\n",
    "\t\t\ttarget_feature = model(style_image).detach()\n",
    "\t\t\tstyle_loss = StyleLoss(target_feature)\n",
    "\t\t\tmodel.add_module('style_loss_{}'.format(i), style_loss)\n",
    "\t\t\tstyle_losses.append(style_loss)\n",
    "\n",
    "\t\t# Insert content loss layer\n",
    "\t\tif name in content_layers:\n",
    "\t\t\ttarget = model(content_image).detach()\n",
    "\t\t\tcontent_loss = ContentLoss(target)\n",
    "\t\t\tmodel.add_module('content_loss_{}'.format(i), content_loss)\n",
    "\t\t\tcontent_losses.append(content_loss)\n",
    "\n",
    "\t# Get rid of unneeded layers after our final losses\n",
    "\tfor i in range(len(model) - 1, -1, -1):\n",
    "\t\tif isinstance(model[i], StyleLoss) or isinstance(model[i], ContentLoss):\n",
    "\t\t\tbreak\n",
    "\n",
    "\tmodel = model[:(i + 1)]\n",
    "\n",
    "\treturn model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sid/miniconda3/envs/md-intro-tutorial/lib/python3.13/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sid/miniconda3/envs/md-intro-tutorial/lib/python3.13/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using LBFGS, Style Weight: 1000, Content Weight: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29564/4032233483.py:37: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
      "/tmp/ipykernel_29564/4032233483.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  self.std = torch.tensor(std).view(-1, 1, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 200 - Style Loss: 19.1936 Content Loss: 424.3852\n",
      "Saved output: output/output_1_LBFGS_SW1000_CW1.jpg\n",
      "\n",
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using Adam, Style Weight: 1000, Content Weight: 1\n",
      "Run 200 - Style Loss: 0.0033 Content Loss: 7.3786\n",
      "Saved output: output/output_1_Adam_SW1000_CW1.jpg\n",
      "\n",
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using LBFGS, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0143 Content Loss: 2.6860\n",
      "Saved output: output/output_1_LBFGS_SW1000_CW5.jpg\n",
      "\n",
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using Adam, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0161 Content Loss: 2.2823\n",
      "Saved output: output/output_1_Adam_SW1000_CW5.jpg\n",
      "\n",
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using LBFGS, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0282 Content Loss: 0.6051\n",
      "Saved output: output/output_1_LBFGS_SW1000_CW10.jpg\n",
      "\n",
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using Adam, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0283 Content Loss: 0.5945\n",
      "Saved output: output/output_1_Adam_SW1000_CW10.jpg\n",
      "\n",
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using LBFGS, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0350 Content Loss: 0.0843\n",
      "Saved output: output/output_1_LBFGS_SW1000_CW20.jpg\n",
      "\n",
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using Adam, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0350 Content Loss: 0.0864\n",
      "Saved output: output/output_1_Adam_SW1000_CW20.jpg\n",
      "\n",
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using LBFGS, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0371 Content Loss: 0.0101\n",
      "Saved output: output/output_1_LBFGS_SW1000_CW50.jpg\n",
      "\n",
      "Processing bear.jpg with Erin-Hanson-Monet's-Bridge.jpg using Adam, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0371 Content Loss: 0.0122\n",
      "Saved output: output/output_1_Adam_SW1000_CW50.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using LBFGS, Style Weight: 1000, Content Weight: 1\n",
      "Run 200 - Style Loss: 0.0024 Content Loss: 3.4120\n",
      "Saved output: output/output_2_LBFGS_SW1000_CW1.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using Adam, Style Weight: 1000, Content Weight: 1\n",
      "Run 200 - Style Loss: 0.0028 Content Loss: 2.7667\n",
      "Saved output: output/output_2_Adam_SW1000_CW1.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using LBFGS, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0072 Content Loss: 1.0877\n",
      "Saved output: output/output_2_LBFGS_SW1000_CW5.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using Adam, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0081 Content Loss: 0.7253\n",
      "Saved output: output/output_2_Adam_SW1000_CW5.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using LBFGS, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0114 Content Loss: 0.2630\n",
      "Saved output: output/output_2_LBFGS_SW1000_CW10.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using Adam, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0114 Content Loss: 0.2599\n",
      "Saved output: output/output_2_Adam_SW1000_CW10.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using LBFGS, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0137 Content Loss: 0.0992\n",
      "Saved output: output/output_2_LBFGS_SW1000_CW20.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using Adam, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0137 Content Loss: 0.0991\n",
      "Saved output: output/output_2_Adam_SW1000_CW20.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using LBFGS, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0159 Content Loss: 0.0245\n",
      "Saved output: output/output_2_LBFGS_SW1000_CW50.jpg\n",
      "\n",
      "Processing building.jpg with bet-you.jpg using Adam, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0159 Content Loss: 0.0253\n",
      "Saved output: output/output_2_Adam_SW1000_CW50.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using LBFGS, Style Weight: 1000, Content Weight: 1\n",
      "Run 200 - Style Loss: 0.0003 Content Loss: 0.4539\n",
      "Saved output: output/output_3_LBFGS_SW1000_CW1.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using Adam, Style Weight: 1000, Content Weight: 1\n",
      "Run 200 - Style Loss: 0.0003 Content Loss: 0.4429\n",
      "Saved output: output/output_3_Adam_SW1000_CW1.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using LBFGS, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0011 Content Loss: 0.1246\n",
      "Saved output: output/output_3_LBFGS_SW1000_CW5.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using Adam, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0011 Content Loss: 0.1178\n",
      "Saved output: output/output_3_Adam_SW1000_CW5.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using LBFGS, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0016 Content Loss: 0.0452\n",
      "Saved output: output/output_3_LBFGS_SW1000_CW10.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using Adam, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0017 Content Loss: 0.0416\n",
      "Saved output: output/output_3_Adam_SW1000_CW10.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using LBFGS, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0021 Content Loss: 0.0121\n",
      "Saved output: output/output_3_LBFGS_SW1000_CW20.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using Adam, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0021 Content Loss: 0.0122\n",
      "Saved output: output/output_3_Adam_SW1000_CW20.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using LBFGS, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0024 Content Loss: 0.0019\n",
      "Saved output: output/output_3_LBFGS_SW1000_CW50.jpg\n",
      "\n",
      "Processing cat-on-table.jpg with head-of-paula-eyles.jpg using Adam, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0024 Content Loss: 0.0029\n",
      "Saved output: output/output_3_Adam_SW1000_CW50.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using LBFGS, Style Weight: 1000, Content Weight: 1\n",
      "Run 200 - Style Loss: 0.0006 Content Loss: 0.7500\n",
      "Saved output: output/output_4_LBFGS_SW1000_CW1.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using Adam, Style Weight: 1000, Content Weight: 1\n",
      "Run 200 - Style Loss: 0.0007 Content Loss: 0.7350\n",
      "Saved output: output/output_4_Adam_SW1000_CW1.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using LBFGS, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0020 Content Loss: 0.1230\n",
      "Saved output: output/output_4_LBFGS_SW1000_CW5.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using Adam, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0020 Content Loss: 0.1278\n",
      "Saved output: output/output_4_Adam_SW1000_CW5.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using LBFGS, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0026 Content Loss: 0.0319\n",
      "Saved output: output/output_4_LBFGS_SW1000_CW10.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using Adam, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0026 Content Loss: 0.0404\n",
      "Saved output: output/output_4_Adam_SW1000_CW10.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using LBFGS, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0029 Content Loss: 0.0079\n",
      "Saved output: output/output_4_LBFGS_SW1000_CW20.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using Adam, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0029 Content Loss: 0.0192\n",
      "Saved output: output/output_4_Adam_SW1000_CW20.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using LBFGS, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0031 Content Loss: 0.0011\n",
      "Saved output: output/output_4_LBFGS_SW1000_CW50.jpg\n",
      "\n",
      "Processing cat-sleeping.jpg with horse-cart.jpg using Adam, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0031 Content Loss: 0.0122\n",
      "Saved output: output/output_4_Adam_SW1000_CW50.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using LBFGS, Style Weight: 1000, Content Weight: 1\n",
      "Run 200 - Style Loss: 0.0007 Content Loss: 0.4723\n",
      "Saved output: output/output_5_LBFGS_SW1000_CW1.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using Adam, Style Weight: 1000, Content Weight: 1\n",
      "Run 200 - Style Loss: 0.0007 Content Loss: 0.4641\n",
      "Saved output: output/output_5_Adam_SW1000_CW1.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using LBFGS, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0015 Content Loss: 0.0751\n",
      "Saved output: output/output_5_LBFGS_SW1000_CW5.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using Adam, Style Weight: 1000, Content Weight: 5\n",
      "Run 200 - Style Loss: 0.0015 Content Loss: 0.0885\n",
      "Saved output: output/output_5_Adam_SW1000_CW5.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using LBFGS, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0018 Content Loss: 0.0290\n",
      "Saved output: output/output_5_LBFGS_SW1000_CW10.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using Adam, Style Weight: 1000, Content Weight: 10\n",
      "Run 200 - Style Loss: 0.0018 Content Loss: 0.0427\n",
      "Saved output: output/output_5_Adam_SW1000_CW10.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using LBFGS, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0020 Content Loss: 0.0100\n",
      "Saved output: output/output_5_LBFGS_SW1000_CW20.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using Adam, Style Weight: 1000, Content Weight: 20\n",
      "Run 200 - Style Loss: 0.0020 Content Loss: 0.0253\n",
      "Saved output: output/output_5_Adam_SW1000_CW20.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using LBFGS, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0023 Content Loss: 0.0021\n",
      "Saved output: output/output_5_LBFGS_SW1000_CW50.jpg\n",
      "\n",
      "Processing cat.jpg with landscape-with-a-palace.jpg using Adam, Style Weight: 1000, Content Weight: 50\n",
      "Run 200 - Style Loss: 0.0023 Content Loss: 0.0181\n",
      "Saved output: output/output_5_Adam_SW1000_CW50.jpg\n",
      "\n",
      "✅ All images processed with both optimizers for different weight values!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "# Constants\n",
    "EPOCHS = 200\n",
    "STYLE_WEIGHTS = [1000, 1000, 1000, 1000, 1000]\n",
    "CONTENT_WEIGHTS = [1, 5, 10, 20, 50]\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Paths\n",
    "STYLE_FOLDER = \"Q3/styles2\"\n",
    "CONTENT_FOLDER = \"Q3/content2\"\n",
    "OUTPUT_FOLDER = \"output\"\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# Load all style and content images\n",
    "style_files = sorted([f for f in os.listdir(STYLE_FOLDER) if f.endswith('.jpg')])\n",
    "content_files = sorted([f for f in os.listdir(CONTENT_FOLDER) if f.endswith('.jpg')])\n",
    "\n",
    "# Load VGG model once\n",
    "cnn = models.vgg19(pretrained=True).features.to(DEVICE).eval()\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(DEVICE)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(DEVICE)\n",
    "\n",
    "def process_image_pair(style_path, content_path, output_prefix, optimizer_type, style_weight, content_weight):\n",
    "    print(f\"Processing {os.path.basename(content_path)} with {os.path.basename(style_path)} using {optimizer_type}, Style Weight: {style_weight}, Content Weight: {content_weight}\")\n",
    "    \n",
    "    # Load images\n",
    "    style_image = load_image(style_path)\n",
    "    content_image = load_image(content_path)\n",
    "    \n",
    "    # Initialize target image\n",
    "    target_image = content_image.clone()\n",
    "    \n",
    "    # Build style transfer model\n",
    "    model, style_losses, content_losses = style_cnn(cnn, DEVICE, \n",
    "        cnn_normalization_mean, cnn_normalization_std, style_image, content_image)\n",
    "    \n",
    "    # Define optimizer\n",
    "    if optimizer_type == 'LBFGS':\n",
    "        optimizer = optim.LBFGS([target_image.requires_grad_()])\n",
    "    else:  # Adam\n",
    "        optimizer = optim.Adam([target_image.requires_grad_()], lr=0.01)\n",
    "    \n",
    "    run = [0]\n",
    "    def closure():\n",
    "        target_image.data.clamp_(0, 1)\n",
    "        optimizer.zero_grad()\n",
    "        model(target_image)\n",
    "        style_score = sum(s.loss for s in style_losses)\n",
    "        content_score = sum(c.loss for c in content_losses)\n",
    "        loss = (style_score * style_weight) + (content_score * content_weight)\n",
    "        loss.backward()\n",
    "        run[0] += 1\n",
    "        if run[0] % 200 == 0:\n",
    "            print(f\"Run {run[0]} - Style Loss: {style_score.item():.4f} Content Loss: {content_score.item():.4f}\")\n",
    "        return loss\n",
    "    \n",
    "    if optimizer_type == 'LBFGS':\n",
    "        while run[0] < EPOCHS:\n",
    "            optimizer.step(closure)\n",
    "    else:  # Adam\n",
    "        for _ in range(EPOCHS):\n",
    "            optimizer.step(closure)\n",
    "    \n",
    "    # Save result\n",
    "    output_path = os.path.join(OUTPUT_FOLDER, f\"{output_prefix}_{optimizer_type}_SW{style_weight}_CW{content_weight}.jpg\")\n",
    "    save_image(target_image, output_path)\n",
    "    print(f\"Saved output: {output_path}\\n\")\n",
    "    \n",
    "    # Free memory\n",
    "    del style_image, content_image, target_image, model, style_losses, content_losses\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "# Process first 5 images with both optimizers and different weight values\n",
    "for i in range(min(5, len(style_files), len(content_files))):\n",
    "    style_path = os.path.join(STYLE_FOLDER, style_files[i])\n",
    "    content_path = os.path.join(CONTENT_FOLDER, content_files[i])\n",
    "    \n",
    "    for style_weight, content_weight in zip(STYLE_WEIGHTS, CONTENT_WEIGHTS):\n",
    "        process_image_pair(style_path, content_path, f\"output_{i+1}\", 'LBFGS', style_weight, content_weight)\n",
    "        torch.cuda.empty_cache()\n",
    "        process_image_pair(style_path, content_path, f\"output_{i+1}\", 'Adam', style_weight, content_weight)\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"✅ All images processed with both optimizers for different weight values!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of Adam and L-BFGS for Style Transfer\n",
    "\n",
    "## Observations\n",
    "\n",
    "### Style Loss:\n",
    "- Adam and L-BFGS resulted in nearly identical style losses across different content weights.\n",
    "- At lower content weights (CW=1), L-BFGS had slightly lower style loss in some cases.\n",
    "- At higher content weights (CW=50), both optimizers achieved near-identical style losses.\n",
    "\n",
    "### Content Loss:\n",
    "- L-BFGS consistently produced slightly lower content loss than Adam.\n",
    "- The gap is more noticeable at lower content weights but diminishes as CW increases.\n",
    "- The trend suggests that L-BFGS preserves content details better.\n",
    "\n",
    "### Effect of Content Weight:\n",
    "- As CW increases, content loss decreases significantly.\n",
    "- At CW=50, both optimizers achieve very low content loss (~0.001–0.025), indicating near-perfect content preservation.\n",
    "\n",
    "## Key Takeaways\n",
    "- **L-BFGS slightly outperforms Adam in preserving content, especially at lower content weights.**\n",
    "- **Both optimizers achieve similar style loss at higher CW, but L-BFGS achieves slightly lower content loss overall.**\n",
    "- **Adam may be preferable for consistency and computational efficiency, while L-BFGS may be better for maximizing content preservation.**\n",
    "\n",
    "### Outputs stored in outputs dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "md-intro-tutorial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
