{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import torchmetrics # Use torchmetrics for mIoU\n",
    "from torchmetrics.segmentation import MeanIoU\n",
    "\n",
    "# --- Configuration ---\n",
    "config = {\n",
    "    \"dataset_path\": \"dataset\", # IMPORTANT: Change this path\n",
    "    \"num_classes\": 13,\n",
    "    \"batch_size\": 8, # Adjust based on your GPU memory\n",
    "    \"epochs\": 30,      # Increase for better convergence\n",
    "    \"learning_rate\": 1e-4,\n",
    "    \"backbone\": \"vgg19\", # or \"vgg19\"\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"wandb_project\": \"FCN_Semantic_Segmentation\",\n",
    "    \"seed\": 45,\n",
    "    \"img_size\": (224, 224)\n",
    "}\n",
    "\n",
    "# --- Reproducibility ---\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "# --- Class Names and Color Map ---\n",
    "CLASS_NAMES = [\n",
    "    \"Unlabeled\", \"Building\", \"Fence\", \"Other\", \"Pedestrian\", \"Pole\",\n",
    "    \"Roadline\", \"Road\", \"Sidewalk\", \"Vegetation\", \"Car\", \"Wall\", \"Traffic sign\"\n",
    "]\n",
    "\n",
    "# Create a color map for visualization (13 distinct colors)\n",
    "# Using tab20 colormap and extending it slightly\n",
    "cmap = plt.get_cmap('tab20', config[\"num_classes\"])\n",
    "COLOR_MAP = [cmap(i)[:3] for i in range(config[\"num_classes\"])]\n",
    "COLOR_MAP_UINT8 = [(int(r*255), int(g*255), int(b*255)) for r, g, b in COLOR_MAP]\n",
    "\n",
    "class SegmentationDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, target_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.image_dir = os.path.join(root_dir, 'images')\n",
    "        self.label_dir = os.path.join(root_dir, 'labels')\n",
    "        # Filter out potential hidden files like .DS_Store\n",
    "        self.image_filenames = sorted([f for f in os.listdir(self.image_dir) if not f.startswith('.')])\n",
    "        self.label_filenames = sorted([f for f in os.listdir(self.label_dir) if not f.startswith('.')])\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform # Keep target_transform if provided\n",
    "\n",
    "        # Ensure images and labels match\n",
    "        assert len(self.image_filenames) == len(self.label_filenames), \\\n",
    "            f\"Mismatch in number of images ({len(self.image_filenames)}) and labels ({len(self.label_filenames)}) in {root_dir}\"\n",
    "        for img_fn, lbl_fn in zip(self.image_filenames, self.label_filenames):\n",
    "             # Check if base names match (e.g., 'img1.png', 'img1.png')\n",
    "             # Or if label has different extension (e.g., 'img1.jpg', 'img1.png')\n",
    "             img_base = os.path.splitext(img_fn)[0]\n",
    "             lbl_base = os.path.splitext(lbl_fn)[0]\n",
    "             assert img_base == lbl_base, \\\n",
    "                 f\"Mismatch filenames (bases): {img_base} (from {img_fn}) and {lbl_base} (from {lbl_fn})\"\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_filenames[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.label_filenames[idx])\n",
    "\n",
    "        try:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            # Load label as grayscale, assuming it contains integer class IDs 0-12\n",
    "            label_pil = Image.open(label_path).convert('L')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading index {idx}: {self.image_filenames[idx]}, {self.label_filenames[idx]}\")\n",
    "            print(e)\n",
    "            raise IOError(f\"Could not load image/label at index {idx}\")\n",
    "\n",
    "\n",
    "        # Apply image transform (ToTensor, Normalize etc.)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Apply label transform (Resize only)\n",
    "        if self.target_transform:\n",
    "            label_pil_resized = self.target_transform(label_pil)\n",
    "        else:\n",
    "             label_pil_resized = label_pil # No resize if no transform provided\n",
    "\n",
    "\n",
    "        # --- Convert resized PIL label to LongTensor ---\n",
    "        # Convert PIL image to numpy array first\n",
    "        label_np = np.array(label_pil_resized, dtype=np.int64)\n",
    "        # Convert numpy array to tensor\n",
    "        label = torch.from_numpy(label_np).long()\n",
    "        # Label should now be a [H, W] tensor with integer class IDs\n",
    "\n",
    "        # Clamp label values to be within num_classes range just in case\n",
    "        # Values outside this range will cause errors in CrossEntropyLoss\n",
    "        label = torch.clamp(label, 0, config[\"num_classes\"] - 1)\n",
    "\n",
    "        # Print unique values for debugging the first few samples\n",
    "        # if idx < 5:\n",
    "        #    print(f\"Sample {idx}: Unique label values after processing: {torch.unique(label)}\")\n",
    "\n",
    "        return image, label\n",
    "# --- Transformations ---\n",
    "# Image transforms: Resize, ToTensor, Normalize\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize(config[\"img_size\"]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Label transforms: Resize (using nearest neighbor), ToTensor\n",
    "# Note: We convert to LongTensor within the dataset __getitem__ for CrossEntropy\n",
    "__\n",
    "label_transform = transforms.Compose([\n",
    "    transforms.Resize(config[\"img_size\"], interpolation=transforms.InterpolationMode.NEAREST),\n",
    "])\n",
    "\n",
    "\n",
    "# --- Dataset Visualization (Task 2.1) ---\n",
    "# --- Dataset Visualization (Task 2.1) ---\n",
    "def visualize_dataset_sample(dataset, index=0):\n",
    "    \"\"\"Visualizes a sample image and its class-specific masks.\"\"\"\n",
    "    image, label_tensor = dataset[index] # Get raw tensor label\n",
    "\n",
    "    # Denormalize image for display\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "    image_display = inv_normalize(image).permute(1, 2, 0).cpu().numpy()\n",
    "    image_display = np.clip(image_display, 0, 1)\n",
    "\n",
    "    label_numpy = label_tensor.cpu().numpy() # Label might be [1, H, W] or [H, W]\n",
    "\n",
    "    # --- FIX: Ensure label_numpy is 2D ---\n",
    "    if label_numpy.ndim == 3 and label_numpy.shape[0] == 1:\n",
    "        # print(f\"DEBUG: Squeezing label_numpy shape from {label_numpy.shape}\")\n",
    "        label_numpy = label_numpy.squeeze(0) # Remove leading dimension if present\n",
    "    elif label_numpy.ndim != 2:\n",
    "         # If it's not 3D with size 1 first, and not 2D, it's an unexpected shape\n",
    "         raise ValueError(f\"Unexpected label shape received in visualize_dataset_sample: {label_numpy.shape}\")\n",
    "    # Now label_numpy is guaranteed to be (H, W)\n",
    "\n",
    "    num_classes = config[\"num_classes\"]\n",
    "    rows = 3\n",
    "    # Calculate columns needed (+2 for original image and combined mask)\n",
    "    cols = (num_classes + 2 + rows - 1) // rows\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(18, 9)) # Adjust figsize if needed\n",
    "\n",
    "    # Plot original image\n",
    "    plt.subplot(rows, cols, 1)\n",
    "    plt.imshow(image_display)\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Plot original mask (colored)\n",
    "    # This uses the now corrected 2D label_numpy\n",
    "    colored_mask = np.zeros((*label_numpy.shape, 3), dtype=np.uint8) # Shape is (H, W, 3)\n",
    "    for i in range(num_classes):\n",
    "         mask = label_numpy == i # mask is (H, W) boolean\n",
    "         colored_mask[mask] = COLOR_MAP_UINT8[i]\n",
    "    plt.subplot(rows, cols, 2)\n",
    "    plt.imshow(colored_mask) # This should now work\n",
    "    plt.title(\"Ground Truth Mask\")\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "    # Plot binary masks for each class\n",
    "    for i in range(num_classes):\n",
    "        binary_mask = (label_numpy == i).astype(np.uint8) * 255\n",
    "        # Calculate subplot index starting from 3\n",
    "        plot_index = i + 3\n",
    "        if plot_index <= rows * cols: # Check if subplot index is valid\n",
    "             plt.subplot(rows, cols, plot_index)\n",
    "             plt.imshow(binary_mask, cmap='gray')\n",
    "             plt.title(f\"{CLASS_NAMES[i]} (ID: {i})\", fontsize=8) # Smaller font if needed\n",
    "             plt.axis('off')\n",
    "        else:\n",
    "            print(f\"Warning: Not enough subplots calculated ({rows}x{cols}) to display class {i+1}/{num_classes}\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- FCN Model Definition ---\n",
    "def get_vgg_backbone(name=\"vgg16\", pretrained=True):\n",
    "    \"\"\"Loads a pretrained VGG backbone\"\"\"\n",
    "    if name == \"vgg16\":\n",
    "        model = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "    elif name == \"vgg19\":\n",
    "         model = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1 if pretrained else None)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported backbone: {name}\")\n",
    "\n",
    "    # Remove the classifier part\n",
    "    features = model.features\n",
    "    # Identify pool3, pool4 layers based on VGG structure\n",
    "    # VGG16: pool3 is layer 16, pool4 is layer 23, pool5 is layer 30\n",
    "    # VGG19: pool3 is layer 18, pool4 is layer 27, pool5 is layer 36\n",
    "    # Use names if available, otherwise indices (check print(model.features))\n",
    "    pool_indices = {'vgg16': {'pool3': 16, 'pool4': 23, 'pool5': 30},\n",
    "                    'vgg19': {'pool3': 18, 'pool4': 27, 'pool5': 36}}\n",
    "    return features, pool_indices[name]\n",
    "\n",
    "\n",
    "class FCN(nn.Module):\n",
    "    def __init__(self, backbone_name, num_classes, variant='FCN-32s', pretrained=True, freeze_backbone=False):\n",
    "        super().__init__()\n",
    "        self.variant = variant\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.features, self.pool_indices = get_vgg_backbone(backbone_name, pretrained)\n",
    "\n",
    "        # Freeze backbone if requested\n",
    "        if freeze_backbone:\n",
    "            print(\"Freezing backbone weights.\")\n",
    "            for param in self.features.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        # Determine feature map depth after pool5\n",
    "        # Run a dummy tensor to find out the number of channels\n",
    "        with torch.no_grad():\n",
    "             dummy_input = torch.zeros(1, 3, config[\"img_size\"][0], config[\"img_size\"][1])\n",
    "             pool5_out_channels = self.features(dummy_input).shape[1]\n",
    "             # Also get pool4 and pool3 channels if needed\n",
    "             pool4_idx = self.pool_indices['pool4']\n",
    "             pool3_idx = self.pool_indices['pool3']\n",
    "             pool4_out_channels = self.features[:pool4_idx+1](dummy_input).shape[1]\n",
    "             pool3_out_channels = self.features[:pool3_idx+1](dummy_input).shape[1]\n",
    "\n",
    "\n",
    "        # Replace VGG classifier with 1x1 convolutions\n",
    "        self.score_pool5 = nn.Conv2d(pool5_out_channels, num_classes, kernel_size=1)\n",
    "\n",
    "        if variant == 'FCN-32s':\n",
    "            # Upsample directly to input size\n",
    "            # Kernel size should be 2 * stride, padding = stride / 2 for bilinear like init\n",
    "            self.upsample32 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=64, stride=32, padding=16, bias=False)\n",
    "            self._initialize_weights(self.upsample32) # Initialize upsampling weights bilinearly\n",
    "\n",
    "        elif variant == 'FCN-16s':\n",
    "            self.score_pool4 = nn.Conv2d(pool4_out_channels, num_classes, kernel_size=1)\n",
    "            self.upsample2_pool5 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "            self.upsample16_combined = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=32, stride=16, padding=8, bias=False)\n",
    "            self._initialize_weights(self.upsample2_pool5, self.upsample16_combined)\n",
    "\n",
    "        elif variant == 'FCN-8s':\n",
    "             self.score_pool4 = nn.Conv2d(pool4_out_channels, num_classes, kernel_size=1)\n",
    "             self.score_pool3 = nn.Conv2d(pool3_out_channels, num_classes, kernel_size=1)\n",
    "\n",
    "             self.upsample2_pool5 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "             self.upsample2_pool4 = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "             self.upsample8_combined = nn.ConvTranspose2d(num_classes, num_classes, kernel_size=16, stride=8, padding=4, bias=False)\n",
    "             self._initialize_weights(self.upsample2_pool5, self.upsample2_pool4, self.upsample8_combined)\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown FCN variant: {variant}\")\n",
    "\n",
    "    def _initialize_weights(self, *layers):\n",
    "        \"\"\"Initialize ConvTranspose2d layers for bilinear upsampling.\"\"\"\n",
    "        for layer in layers:\n",
    "            if isinstance(layer, nn.ConvTranspose2d):\n",
    "                # Bilinear initialization\n",
    "                # From: https://github.com/shelhamer/fcn.berkeleyvision.org/blob/master/surgery.py\n",
    "                factor = (layer.kernel_size[0] + 1) // 2\n",
    "                if layer.kernel_size[0] % 2 == 1:\n",
    "                    center = factor - 1\n",
    "                else:\n",
    "                    center = factor - 0.5\n",
    "                og = np.ogrid[:layer.kernel_size[0], :layer.kernel_size[1]]\n",
    "                filt = (1 - abs(og[0] - center) / factor) * \\\n",
    "                       (1 - abs(og[1] - center) / factor)\n",
    "                weight = np.zeros((layer.in_channels, layer.out_channels,\n",
    "                                   layer.kernel_size[0], layer.kernel_size[1]),\n",
    "                                  dtype=np.float32)\n",
    "                weight[range(layer.in_channels), range(layer.out_channels), :, :] = filt\n",
    "                layer.weight.data.copy_(torch.from_numpy(weight))\n",
    "                if layer.bias is not None:\n",
    "                     nn.init.constant_(layer.bias.data, 0)\n",
    "\n",
    "        # Initialize score layers with zeros (as suggested in paper)\n",
    "        if hasattr(self, 'score_pool5'): nn.init.zeros_(self.score_pool5.weight) ; nn.init.zeros_(self.score_pool5.bias)\n",
    "        if hasattr(self, 'score_pool4'): nn.init.zeros_(self.score_pool4.weight) ; nn.init.zeros_(self.score_pool4.bias)\n",
    "        if hasattr(self, 'score_pool3'): nn.init.zeros_(self.score_pool3.weight) ; nn.init.zeros_(self.score_pool3.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        input_size = x.shape[-2:] # H, W\n",
    "        pool3_idx = self.pool_indices['pool3']\n",
    "        pool4_idx = self.pool_indices['pool4']\n",
    "        pool5_idx = self.pool_indices['pool5'] # End of features usually\n",
    "\n",
    "        # Pass through backbone, saving intermediate features if needed\n",
    "        pool3_feat, pool4_feat, pool5_feat = None, None, None\n",
    "        current_feat = x\n",
    "        for i, layer in enumerate(self.features):\n",
    "            current_feat = layer(current_feat)\n",
    "            if i == pool3_idx and self.variant in ['FCN-8s']:\n",
    "                pool3_feat = current_feat\n",
    "            elif i == pool4_idx and self.variant in ['FCN-16s', 'FCN-8s']:\n",
    "                pool4_feat = current_feat\n",
    "            elif i == pool5_idx: # Assumes pool5 is the last layer of features\n",
    "                pool5_feat = current_feat\n",
    "                if self.variant == 'FCN-32s': # Stop early if we only need pool5\n",
    "                     break\n",
    "\n",
    "        # --- Score and Upsample ---\n",
    "        score5 = self.score_pool5(pool5_feat)\n",
    "\n",
    "        if self.variant == 'FCN-32s':\n",
    "            out = self.upsample32(score5)\n",
    "            # Crop to input size (ConvTranspose2d might add padding)\n",
    "            out = self._crop(out, input_size)\n",
    "            return out\n",
    "\n",
    "        elif self.variant == 'FCN-16s':\n",
    "            score4 = self.score_pool4(pool4_feat)\n",
    "            upsampled_score5 = self.upsample2_pool5(score5)\n",
    "\n",
    "            # Crop score4 to match upsampled_score5 size\n",
    "            score4_cropped = self._crop(score4, upsampled_score5.shape[-2:])\n",
    "            combined16 = score4_cropped + upsampled_score5\n",
    "\n",
    "            out = self.upsample16_combined(combined16)\n",
    "            out = self._crop(out, input_size)\n",
    "            return out\n",
    "\n",
    "        elif self.variant == 'FCN-8s':\n",
    "             score4 = self.score_pool4(pool4_feat)\n",
    "             score3 = self.score_pool3(pool3_feat)\n",
    "\n",
    "             upsampled_score5 = self.upsample2_pool5(score5)\n",
    "             score4_cropped = self._crop(score4, upsampled_score5.shape[-2:])\n",
    "             combined_pool45 = score4_cropped + upsampled_score5\n",
    "\n",
    "             upsampled_pool45 = self.upsample2_pool4(combined_pool45)\n",
    "             score3_cropped = self._crop(score3, upsampled_pool45.shape[-2:])\n",
    "             combined_pool345 = score3_cropped + upsampled_pool45\n",
    "\n",
    "             out = self.upsample8_combined(combined_pool345)\n",
    "             out = self._crop(out, input_size)\n",
    "             return out\n",
    "\n",
    "    def _crop(self, tensor, target_size):\n",
    "        \"\"\"Center crop a tensor to target size (H, W).\"\"\"\n",
    "        _, _, H, W = tensor.shape\n",
    "        th, tw = target_size\n",
    "        if H == th and W == tw:\n",
    "            return tensor\n",
    "        # Calculate crop start indices\n",
    "        h_start = (H - th) // 2\n",
    "        w_start = (W - tw) // 2\n",
    "        return tensor[:, :, h_start:h_start+th, w_start:w_start+tw]\n",
    "\n",
    "\n",
    "# --- Training and Validation Functions ---\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device, epoch, variant, freeze_status):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} Train ({variant}, {freeze_status})\")\n",
    "    for images, masks in pbar:\n",
    "        images, masks = images.to(device), masks.to(device).long() # Ensure masks are Long\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "        wandb.log({f\"Train/Batch Loss ({variant}, {freeze_status})\": loss.item()})\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    print(f\"Epoch {epoch+1} Train Loss ({variant}, {freeze_status}): {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def validate(model, dataloader, criterion, metric, device, epoch, variant, freeze_status):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    num_batches = len(dataloader)\n",
    "    metric.reset()\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1} Val ({variant}, {freeze_status})\")\n",
    "    with torch.no_grad():\n",
    "        for images, masks in pbar:\n",
    "            images, masks = images.to(device), masks.to(device).long()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # print(f\"Preds shape: {preds.shape}, Masks shape: {masks.shape}\")\n",
    "            metric.update(preds, masks) # Metric on CPU\n",
    "\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    final_miou = metric.compute()\n",
    "    print(f\"Epoch {epoch+1} Val Loss ({variant}, {freeze_status}): {avg_loss:.4f}, mIoU: {final_miou}\")\n",
    "    return avg_loss, final_miou\n",
    "\n",
    "\n",
    "# --- Evaluation on Test Set ---\n",
    "def evaluate_test_set(model, dataloader, metric, device, variant, freeze_status):\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    pbar = tqdm(dataloader, desc=f\"Evaluating Test Set ({variant}, {freeze_status})\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, masks in pbar:\n",
    "            images, masks = images.to(device), masks.to(device).long()\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            # print(f\"Preds shape: {preds.shape}, Masks shape: {masks.shape}\")\n",
    "            metric.update(preds, masks) # Metric on CPU\n",
    "\n",
    "    final_miou = metric.compute()\n",
    "    print(f\"Test mIoU ({variant}, {freeze_status}): {final_miou}\")\n",
    "    return final_miou\n",
    "\n",
    "# --- Visualization of Predictions ---\n",
    "def visualize_predictions(model, dataloader, device, num_samples=5, variant=None, freeze_status=None):\n",
    "    model.eval()\n",
    "    inv_normalize = transforms.Normalize(\n",
    "        mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "        std=[1/0.229, 1/0.224, 1/0.225]\n",
    "    )\n",
    "\n",
    "    samples_shown = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks_gt in dataloader:\n",
    "            if samples_shown >= num_samples:\n",
    "                break\n",
    "\n",
    "            images, masks_gt = images.to(device), masks_gt.to(device).long()\n",
    "            outputs = model(images)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            for i in range(images.size(0)):\n",
    "                if samples_shown >= num_samples:\n",
    "                    break\n",
    "\n",
    "                img_display = inv_normalize(images[i]).permute(1, 2, 0).cpu().numpy()\n",
    "                img_display = np.clip(img_display, 0, 1)\n",
    "\n",
    "                gt_mask_numpy = masks_gt[i].cpu().numpy()\n",
    "                pred_mask_numpy = preds[i].cpu().numpy()\n",
    "\n",
    "                # Colorize masks\n",
    "                gt_colored = np.zeros((*gt_mask_numpy.shape, 3), dtype=np.uint8)\n",
    "                pred_colored = np.zeros((*pred_mask_numpy.shape, 3), dtype=np.uint8)\n",
    "                for c_id in range(config[\"num_classes\"]):\n",
    "                    gt_colored[gt_mask_numpy == c_id] = COLOR_MAP_UINT8[c_id]\n",
    "                    pred_colored[pred_mask_numpy == c_id] = COLOR_MAP_UINT8[c_id]\n",
    "\n",
    "                plt.figure(figsize=(12, 4))\n",
    "                plt.subplot(1, 3, 1)\n",
    "                plt.imshow(img_display)\n",
    "                plt.title(\"Input Image\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 2)\n",
    "                plt.imshow(gt_colored)\n",
    "                plt.title(\"Ground Truth Mask\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.subplot(1, 3, 3)\n",
    "                plt.imshow(pred_colored)\n",
    "                plt.title(f\"Predicted Mask ({variant}, {freeze_status})\")\n",
    "                plt.axis('off')\n",
    "\n",
    "                plt.suptitle(f\"Sample {samples_shown + 1}\")\n",
    "                plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for suptitle\n",
    "                # Save or show plot\n",
    "                save_filename = f\"prediction_{variant}_{freeze_status}_sample_{samples_shown+1}.png\"\n",
    "                plt.savefig(save_filename)\n",
    "                print(f\"Saved prediction visualization: {save_filename}\")\n",
    "                # plt.show() # Comment out if running non-interactively\n",
    "                plt.close()\n",
    "\n",
    "\n",
    "                samples_shown += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Setup ---\n",
    "device = config[\"device\"]\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load Data ---\n",
    "# Load the FULL training dataset first\n",
    "full_train_dataset = SegmentationDataset(\n",
    "    root_dir=os.path.join(config[\"dataset_path\"], \"train\"),\n",
    "    transform=img_transform,\n",
    "    target_transform=label_transform\n",
    ")\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.8 # 80% for training, 20% for validation\n",
    "n_total = len(full_train_dataset)\n",
    "n_train = int(n_total * train_ratio)\n",
    "n_val = n_total - n_train\n",
    "\n",
    "print(f\"Splitting the original training data ({n_total} samples) into:\")\n",
    "print(f\"  - Training set: {n_train} samples\")\n",
    "print(f\"  - Validation set: {n_val} samples\")\n",
    "\n",
    "# Split the dataset using a fixed generator for reproducibility\n",
    "generator = torch.Generator().manual_seed(config[\"seed\"])\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    full_train_dataset, [n_train, n_val], generator=generator\n",
    ")\n",
    "\n",
    "# Load the test dataset as before\n",
    "test_dataset = SegmentationDataset(\n",
    "    root_dir=os.path.join(config[\"dataset_path\"], \"test\"),\n",
    "    transform=img_transform,\n",
    "    target_transform=label_transform\n",
    ")\n",
    "\n",
    "# Create DataLoaders for the subsets and the test set\n",
    "train_loader = DataLoader(train_subset, batch_size=config[\"batch_size\"], shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_subset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True) # No shuffle for validation\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Update print statement for clarity\n",
    "print(f\"DataLoaders created: Train ({len(train_subset)} samples), Val ({len(val_subset)} samples), Test ({len(test_dataset)} samples)\")\n",
    "\n",
    "\n",
    "# --- Visualize Dataset Sample (Task 2.1) ---\n",
    "print(\"\\nVisualizing one dataset sample and its class masks (from original training set)...\")\n",
    "# Visualize from the original full dataset or the train_subset\n",
    "# Using full_train_dataset ensures we can pick any index before split\n",
    "for i in range(1):\n",
    "    visualize_dataset_sample(full_train_dataset, index=i) # Visualize first 10 samples\n",
    "    plt.show() # Show each plot interactively\n",
    "\n",
    "# --- Training Loop ---\n",
    "# ... (The rest of the script remains the same) ...\n",
    "\n",
    "variants = ['FCN-32s', 'FCN-16s', 'FCN-8s']\n",
    "freeze_options = [True, False] # True: Freeze backbone, False: Fine-tune all\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Ignores class 0 by default if needed, check docs\n",
    "# Use TorchMetrics for mIoU\n",
    "metric = MeanIoU(num_classes=config[\"num_classes\"], per_class= False, input_format = 'index', include_background=False).to(device)\n",
    "\n",
    "for freeze_backbone in freeze_options:\n",
    "    freeze_status = \"Frozen\" if freeze_backbone else \"Finetuned\"\n",
    "    for variant in variants:\n",
    "        print(f\"\\n--- Training {variant} with {freeze_status} Backbone ---\")\n",
    "\n",
    "        # --- Initialize wandb ---\n",
    "        run = wandb.init(\n",
    "            project=config[\"wandb_project\"],\n",
    "            config=config,\n",
    "            name=f\"{variant}-{freeze_status}-{config['backbone']}-e{config['epochs']}-lr{config['learning_rate']}\",\n",
    "            reinit=True # Allows multiple init calls in one script\n",
    "        )\n",
    "        wandb.config.update({\"variant\": variant, \"freeze_backbone\": freeze_backbone})\n",
    "\n",
    "\n",
    "        # --- Model, Optimizer ---\n",
    "        model = FCN(\n",
    "            backbone_name=config[\"backbone\"],\n",
    "            num_classes=config[\"num_classes\"],\n",
    "            variant=variant,\n",
    "            pretrained=True,\n",
    "            freeze_backbone=freeze_backbone\n",
    "        ).to(device)\n",
    "\n",
    "        # Adjust optimizer based on freeze status (optional: differential learning rate)\n",
    "        if freeze_backbone:\n",
    "                optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=config[\"learning_rate\"])\n",
    "        else:\n",
    "                optimizer = optim.Adam(model.parameters(), lr=config[\"learning_rate\"])\n",
    "\n",
    "\n",
    "        best_val_miou = -1.0\n",
    "        model_save_path = f\"best_model_{variant}_{freeze_status}.pth\"\n",
    "\n",
    "        # --- Epoch Loop ---\n",
    "        for epoch in range(config[\"epochs\"]):\n",
    "            train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, variant, freeze_status)\n",
    "            val_loss, val_miou = validate(model, val_loader, criterion, metric, device, epoch, variant, freeze_status)\n",
    "\n",
    "            wandb.log({\n",
    "                f\"Val/Epoch Loss\": val_loss,\n",
    "                f\"Val/Epoch mIoU\": val_miou,\n",
    "                f\"Train/Epoch Loss\": train_loss,\n",
    "                \"epoch\": epoch + 1\n",
    "            })\n",
    "            # use mean mIoU for best model selection\n",
    "            val_miou = val_miou.mean().item() if isinstance(val_miou, torch.Tensor) else val_miou\n",
    "            \n",
    "            # Save best model based on validation mIoU\n",
    "            if val_miou > best_val_miou:\n",
    "                best_val_miou = val_miou\n",
    "                torch.save(model.state_dict(), model_save_path)\n",
    "                print(f\"Saved best model to {model_save_path} (mIoU: {best_val_miou:.4f})\")\n",
    "                wandb.save(model_save_path) # Save model artifact to wandb\n",
    "\n",
    "\n",
    "        # --- Final Evaluation and Visualization ---\n",
    "        print(f\"\\n--- Evaluating {variant} ({freeze_status}) on Test Set ---\")\n",
    "        # Load best model\n",
    "        if os.path.exists(model_save_path):\n",
    "            model.load_state_dict(torch.load(model_save_path))\n",
    "        else:\n",
    "            print(f\"Warning: Best model file {model_save_path} not found. Evaluating with the last state.\")\n",
    "\n",
    "        test_miou = evaluate_test_set(model, test_loader, metric, device, variant, freeze_status)\n",
    "        wandb.log({f\"Test/Final mIoU ({variant}, {freeze_status})\": test_miou})\n",
    "        wandb.summary[f\"best_val_miou_{variant}_{freeze_status}\"] = best_val_miou\n",
    "        wandb.summary[f\"final_test_miou_{variant}_{freeze_status}\"] = test_miou\n",
    "\n",
    "        print(f\"\\n--- Visualizing Predictions for {variant} ({freeze_status}) ---\")\n",
    "        visualize_predictions(model, test_loader, device, num_samples=5, variant=variant, freeze_status=freeze_status)\n",
    "\n",
    "        run.finish() # Finish wandb run for this configuration\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Explanation and Comparison\n",
    "\n",
    "**Summary of Test Set mIoU Results:**\n",
    "\n",
    "| Name                                | variant | freeze_backbone | backbone | Runtime (s) | final_test_miou   |\n",
    "| :---------------------------------- | :------ | :-------------- | :------- | :---------- | :---------------- |\n",
    "| FCN-8s-Finetuned-vgg19-e30-lr0.0001 | FCN-8s  | false           | vgg19    | 973         | 0.2632            |\n",
    "| FCN-16s-Finetuned-vgg19-e30-lr0.0001 | FCN-16s | false           | vgg19    | 972         | 0.2553            |\n",
    "| FCN-32s-Finetuned-vgg19-e30-lr0.0001 | FCN-32s | false           | vgg19    | 980         | 0.2465            |\n",
    "| FCN-8s-Frozen-vgg19-e30-lr0.0001    | FCN-8s  | true            | vgg19    | 406         | 0.2267            |\n",
    "| FCN-16s-Frozen-vgg19-e30-lr0.0001   | FCN-16s | true            | vgg19    | 414         | 0.2139            |\n",
    "| FCN-32s-Frozen-vgg19-e30-lr0.0001   | FCN-32s | true            | vgg19    | 421         | 0.1927            |\n",
    "\n",
    "*(Note: mIoU values are rounded to four decimal places for clarity)*\n",
    "\n",
    "**Differences between FCN-32s, FCN-16s, and FCN-8s:**\n",
    "\n",
    "The core difference between these FCN variants lies in how they combine coarse, high-level semantic information from deep layers with finer, spatial information from shallower layers using skip connections before upsampling back to the original image resolution:\n",
    "\n",
    "*   **FCN-32s:** This variant uses only the output from the *final* convolutional layer of the VGG19 backbone (output stride 32). It applies a classifier (1x1 conv) and then performs a single, large 32x bilinear upsampling to reach the input image size. It lacks access to finer details from earlier layers, resulting in coarser segmentation maps.\n",
    "*   **FCN-16s:** This variant improves upon FCN-32s by incorporating information from an earlier layer. It takes the stride 32 prediction, upsamples it 2x, and fuses it (element-wise addition) with predictions made from the `pool4` layer features (output stride 16). This combined map is then upsampled 16x. By adding `pool4` features, it captures more spatial detail than FCN-32s.\n",
    "*   **FCN-8s:** This variant further refines the process by adding another skip connection. It takes the stride 16 fused map (from the FCN-16s process before final upsampling), upsamples it 2x, and fuses it with predictions made from the `pool3` layer features (output stride 8). This final fused map is then upsampled 8x. By incorporating information from `pool3`, `pool4`, and the final layer, FCN-8s leverages features at multiple spatial resolutions, enabling potentially more accurate localization and finer segmentation boundaries.\n",
    "\n",
    "**Segmentation Performance Discussion:**\n",
    "\n",
    "Based on the `final_test_miou` reported in the table:\n",
    "\n",
    "*   **Trend within Variants:** For both the *frozen* and *finetuned* scenarios, the performance consistently improves as more skip connections are introduced:\n",
    "    *   Frozen: FCN-8s (0.2267) > FCN-16s (0.2139) > FCN-32s (0.1927)\n",
    "    *   Finetuned: FCN-8s (0.2632) > FCN-16s (0.2553) > FCN-32s (0.2465)\n",
    "*   **Interpretation:** The FCN-8s variant achieves the highest mIoU in both settings, indicating that combining feature maps from earlier layers (`pool3`, `pool4`) with the final layer's semantic information leads to the best segmentation results on this dataset, according to the mIoU metric. The progressive addition of finer spatial details helps refine the coarse predictions from the deeper layers. FCN-32s consistently performs the worst, highlighting the limitation of relying solely on the deepest layer's output.\n",
    "\n",
    "**Comparison: Frozen vs. Finetuned Backbone:**\n",
    "\n",
    "Comparing the performance of models with a frozen backbone versus a finetuned backbone reveals significant differences:\n",
    "\n",
    "*   **Performance:** Finetuning the VGG19 backbone yields substantially better results across all FCN variants compared to keeping the backbone frozen.\n",
    "    *   FCN-32s sees a relative mIoU improvement of approximately **28%** (0.2465 vs 0.1927).\n",
    "    *   FCN-16s sees a relative mIoU improvement of approximately **19%** (0.2553 vs 0.2139).\n",
    "    *   FCN-8s sees a relative mIoU improvement of approximately **16%** (0.2632 vs 0.2267).\n",
    "*   **Segmentation Quality:** The higher mIoU scores strongly suggest that finetuning leads to superior segmentation quality. By allowing the backbone weights to be updated, the model can better adapt the ImageNet-pretrained features to the specific characteristics and classes of the target semantic segmentation dataset. This likely translates to more accurate pixel classifications and better delineation of object boundaries (although visual inspection of predictions would be needed to confirm this qualitatively).\n",
    "*   **Training Time:** Finetuning comes at the cost of significantly increased training time. As seen in the `Runtime` column, finetuned models took more than double the time to train compared to their frozen counterparts (e.g., ~970s vs ~410s). This is because gradients must be backpropagated through the entire network, including the large VGG19 backbone, requiring more computation per iteration.\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "The experimental results indicate that **FCN-8s with a finetuned VGG19 backbone** provides the best performance on this dataset, achieving a final test mIoU of **0.2632**. The inclusion of multi-level skip connections (as in FCN-8s) and the adaptation of backbone features via finetuning are both crucial factors for achieving higher accuracy in semantic segmentation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
